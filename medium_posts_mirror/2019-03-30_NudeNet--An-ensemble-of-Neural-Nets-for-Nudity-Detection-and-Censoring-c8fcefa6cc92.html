<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>NudeNet: An ensemble of Neural Nets for Nudity Detection and Censoring</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">NudeNet: An ensemble of Neural Nets for Nudity Detection and Censoring</h1>
</header>
<section data-field="subtitle" class="p-summary">
Part 2: Exposed part detection and censoring.
</section>
<section data-field="body" class="e-content">
<section name="279b" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="fa15" id="fa15" class="graf graf--h3 graf--leading graf--title">NudeNet: An ensemble of Neural Nets for Nudity Detection and Censoring</h3><h3 name="ed49" id="ed49" class="graf graf--h3 graf-after--h3">Part 2: Exposed part detection and censoring.</h3><blockquote name="a5ca" id="a5ca" class="graf graf--blockquote graf-after--h3"><strong class="markup--strong markup--blockquote-strong">The Why</strong>: A major draw back of image classification is not having fine grained control. If we want to blur the exposed parts/ have fine -grained control of which type of images we want to allow, we need Object Detection. <strong class="markup--strong markup--blockquote-strong">This is a first of it’s kind open-source project</strong>, which I hope will be helpful to the community.</blockquote><p name="f41f" id="f41f" class="graf graf--p graf-after--blockquote">If some one wants to allow images with exposed chest or buttocks but not images with exposed genitalia, or some other combination, doing this solely with image classification is not possible. Since, there are no creative ways of obtaining the data-set for this task, I make use of the data collected by <a href="https://github.com/Kadantte" data-href="https://github.com/Kadantte" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Jae Jin</a> and team. For more information on this or to contribute to the development of the dataset, please contact Jae Jin or join their team’s discord server (Discord Tags: 0131#2628 or Jae Jin#0256).</p><p name="8533" id="8533" class="graf graf--p graf-after--p">With a total of 5789 images, the distribution of the number of labels is as follows</p><figure name="b92f" id="b92f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 353px; max-height: 155px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.9%;"></div><img class="graf-image" data-image-id="1*S8pBYOo7yurPVbcvz-p-Aw.png" data-width="353" data-height="155" src="https://cdn-images-1.medium.com/max/800/1*S8pBYOo7yurPVbcvz-p-Aw.png"></div><figcaption class="imageCaption">Total data available for training</figcaption></figure><p name="bb9d" id="bb9d" class="graf graf--p graf-after--figure">With this data, I use the image augmentation library <a href="https://github.com/albu/albumentations" data-href="https://github.com/albu/albumentations" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">albumentations</a> for adding some random blur, flips etc to the dataset.</p><p name="955c" id="955c" class="graf graf--p graf-after--p">Since, there is significant class imbalance in the data, I chose to use RetinaNet by FAIR for object detection. RetinaNet uses a variation of cross entropy loss called Focal Loss, which is designed to increase the performance of one-stage object detection.</p><p name="f815" id="f815" class="graf graf--p graf-after--p">Using, resnet-101 as the backend, the model achieves the following scores on the test data.</p><figure name="c3b6" id="c3b6" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 632px; max-height: 145px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.900000000000002%;"></div><img class="graf-image" data-image-id="1*TPuOHqY6lHtgu3Za-mGYlA.png" data-width="632" data-height="145" src="https://cdn-images-1.medium.com/max/800/1*TPuOHqY6lHtgu3Za-mGYlA.png"></div></figure><h3 name="0b57" id="0b57" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Evaluating the model:</strong></h3><p name="56d3" id="56d3" class="graf graf--p graf-after--h3">Since, <strong class="markup--strong markup--p-strong">this model can be used for both nudity detection and censoring</strong>, I test the model with the same data I used to test the classifier. If in an image, any of the labels “BUTTOCKS_EXPOSED”, “*_GENETALIA_EXPOSED”, “F_BREAST_EXPOSED” are found, we label the image as “nude” and if none of these are found, we label the image as “safe”. This labels mapping is chosen based on test data. For eg: In the test dataset, images with exposed Male Breast or exposed Belly, are present in the “sfw” category.</p><figure name="1aaa" id="1aaa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 437px; max-height: 122px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.900000000000002%;"></div><img class="graf-image" data-image-id="1*C25E8ad1DRuBZ1WcKkB0VA.png" data-width="437" data-height="122" src="https://cdn-images-1.medium.com/max/800/1*C25E8ad1DRuBZ1WcKkB0VA.png"></div><figcaption class="imageCaption">Precision and Recall of NudeNet’s Detector</figcaption></figure><p name="bade" id="bade" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">NudeNet’s Detector performs better than Yahoo’s Open NSFW, GantMan’s nsfw_model and NudeNet’s classifier in identifying porn.</strong></p><p name="b388" id="b388" class="graf graf--p graf-after--p">I also, include a censor function in this class, which censor’s the private parts in a nsfw image.</p><p name="0ffd" id="0ffd" class="graf graf--p graf-after--p">For example, for the image <strong class="markup--strong markup--p-strong">(NSFW)</strong> <a href="https://i.imgur.com/rga6845.jpg" data-href="https://i.imgur.com/rga6845.jpg" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://i.imgur.com/rga6845.jpg,</a> the following is the output of NudeNet’s censor function.</p><figure name="18fa" id="18fa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*7KOb6bbSupcrzxiXkTUDCg.jpeg" data-width="852" data-height="480" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*7KOb6bbSupcrzxiXkTUDCg.jpeg"></div><figcaption class="imageCaption">Censored image, using NudeDetector.</figcaption></figure><p name="1f66" id="1f66" class="graf graf--p graf-after--figure">The project can be found at <a href="https://github.com/bedapudi6788/NudeNet" data-href="https://github.com/bedapudi6788/NudeNet" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://github.com/bedapudi6788/NudeNet</a></p><p name="df4c" id="df4c" class="graf graf--p graf-after--p">The pre-trained models at <a href="https://github.com/bedapudi6788/NudeNet-models/" data-href="https://github.com/bedapudi6788/NudeNet-models/" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://github.com/bedapudi6788/NudeNet-models/</a></p><p name="675e" id="675e" class="graf graf--p graf-after--p">To install and use NudeNet, take a look at the following snippet.</p><pre name="b4ee" id="b4ee" class="graf graf--pre graf-after--p"># installing the project<br>pip install git+<a href="https://github.com/bedapudi6788/NudeNet" data-href="https://github.com/bedapudi6788/NudeNet" class="markup--anchor markup--pre-anchor" rel="nofollow noopener noopener" target="_blank">https://github.com/bedapudi6788/NudeNet</a></pre><pre name="04e2" id="04e2" class="graf graf--pre graf-after--pre graf--trailing"><code class="markup--code markup--pre-code"># Using the classifier<br>from NudeNet import NudeClassifier<br>classifier = NudeClassifier(&#39;classifier_checkpoint_path&#39;)<br>classifier.classify(&#39;path_to_nude_image&#39;)</code></pre></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@praneethbedapudi" class="p-author h-card">Praneeth Bedapudi</a> on <a href="https://medium.com/p/c8fcefa6cc92"><time class="dt-published" datetime="2019-03-30T07:22:42.721Z">March 30, 2019</time></a>.</p><p><a href="https://medium.com/@praneethbedapudi/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-c8fcefa6cc92" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on February 21, 2020.</p></footer></article></body></html>