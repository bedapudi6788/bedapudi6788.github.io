<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>DeepCorrection 1: Sentence Segmentation of unpunctuated text.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">DeepCorrection 1: Sentence Segmentation of unpunctuated text.</h1>
</header>
<section data-field="subtitle" class="p-summary">
Sentence segmentation or Sentence boundary Detection is one of the foremost problems of NLP that is considered to be solved. While working…
</section>
<section data-field="body" class="e-content">
<section name="0b22" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0723" id="0723" class="graf graf--h3 graf--leading graf--title">DeepCorrection 1: Sentence Segmentation of unpunctuated text.</h3><p name="56cc" id="56cc" class="graf graf--p graf-after--h3">Sentence segmentation or Sentence boundary Detection is one of the foremost problems of NLP that is considered to be solved. While working on a text correction module that includes punctuation correction, spell correction and common grammar error correction, I realised that to do any of these my model(s) should be able to correctly segment the input text.</p><p name="5084" id="5084" class="graf graf--p graf-after--p">There are various libraries including some of the most popular ones like NLTK, Spacy, Stanford CoreNLP that that provide excellent, easy to use functions for sentence segmentation. Before you start thinking that this is just another survey post that adds nothing new to the topic, let’s take a look at how these libraries segment the text “I am Batman. I live in Gotham.”</p><figure name="a330" id="a330" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 455px; max-height: 68px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 14.899999999999999%;"></div><img class="graf-image" data-image-id="1*yLazlEtIrH9NfZNtDD_2Cw.png" data-width="455" data-height="68" src="https://cdn-images-1.medium.com/max/800/1*yLazlEtIrH9NfZNtDD_2Cw.png"></div><figcaption class="imageCaption">NLTK works perfectly on this text.</figcaption></figure><figure name="1654" id="1654" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 403px; max-height: 94px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 23.3%;"></div><img class="graf-image" data-image-id="1*8jva-83166cSGhgpe6RolA.png" data-width="403" data-height="94" src="https://cdn-images-1.medium.com/max/800/1*8jva-83166cSGhgpe6RolA.png"></div><figcaption class="imageCaption">So does Spacy.</figcaption></figure><figure name="8859" id="8859" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 298px; max-height: 100px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 33.6%;"></div><img class="graf-image" data-image-id="1*yKRu0ldxwMosS1ZdA0MgOw.png" data-width="298" data-height="100" src="https://cdn-images-1.medium.com/max/800/1*yKRu0ldxwMosS1ZdA0MgOw.png"></div><figcaption class="imageCaption">CoreNLP too, works perfectly.</figcaption></figure><p name="66fa" id="66fa" class="graf graf--p graf-after--figure">Now, let’s see how these algorithms deal with the text “I am Batman I live in Gotham” which is but a small modification of our original sentence.</p><figure name="34a9" id="34a9" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 402px; max-height: 91px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.6%;"></div><img class="graf-image" data-image-id="1*7SJmSUxJiS8HYnm_9jXIeQ.png" data-width="402" data-height="91" src="https://cdn-images-1.medium.com/max/800/1*7SJmSUxJiS8HYnm_9jXIeQ.png"></div><figcaption class="imageCaption">NLTK and Spacy fails (to no one’s surprise)</figcaption></figure><figure name="2da3" id="2da3" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 251px; max-height: 42px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 16.7%;"></div><img class="graf-image" data-image-id="1*-CO5CQS80jv3LOW7LJyD1w.png" data-width="251" data-height="42" src="https://cdn-images-1.medium.com/max/800/1*-CO5CQS80jv3LOW7LJyD1w.png"></div><figcaption class="imageCaption">and so does CoreNLP</figcaption></figure><p name="07cd" id="07cd" class="graf graf--p graf-after--figure">This is of course, well known and expected behaviour since all the famous modules use either statistical models or heavily language dependent patterns to perform sentence boundary detection. These libraries work exactly as they are supposed to i.e: work near perfectly on perfectly formatted text and fail miserably on text with bad punctuations, wrong capitalisations etc.</p><p name="e502" id="e502" class="graf graf--p graf-after--p">So, I decided to give this a try. My prerequisites were that</p><ol class="postList"><li name="8f4b" id="8f4b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">The Data:</strong> I didn’t want to spend more than a day of my time on this project. So, no manual annotations. i.e: Automatic generation of data.</li><li name="cdf1" id="cdf1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">The Model:</strong> This project is strictly for testing purpose. i.e: Figure out and adopt existing architectures.</li><li name="ec5c" id="ec5c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">The Machine:</strong> Since, I don’t want this model to be restricted to particular domains, the training data needs to be huge. Being the cheapskate I am, I didn’t want to spend more than 5 dollars for this. <a href="https://zerosix.ai/" data-href="https://zerosix.ai/" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank">https://zerosix.ai/</a> came to my help with their amazing pricing. ($0.50 per hour for 1080Ti)</li></ol><p name="bdc2" id="bdc2" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Obtaining the Data:</strong></p><p name="46c5" id="46c5" class="graf graf--p graf-after--p">My idea was to obtain a set of gold standard english sentences and combine them randomly to generate my training data. But Obtaining a large multi-domain corpus of gold standard sentences proved tough and I decided to go with wikipedia dump segmented using Spacy/ CoreNLP to get the sentences. But again, this didn’t feel right, since 1. Wikipedia have a very few examples of general conversional data and first person data. 2. Spacy/ CoreNLP can’t do 100% perfect segmentation. So, my data won’t be no where near gold standard.</p><p name="cdc1" id="cdc1" class="graf graf--p graf-after--p">This is when I remembered about <a href="https://tatoeba.org/eng" data-href="https://tatoeba.org/eng" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://tatoeba.org/eng</a>, an open data initiative aimed at translation and speech recognition. Their english corpus has closer to one million sentences covering a broad variety of writing styles, which can be used to generate huge amount of training data. The data for all the languages can be downloaded from <a href="http://downloads.tatoeba.org/exports/sentences.tar.bz2" data-href="http://downloads.tatoeba.org/exports/sentences.tar.bz2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://downloads.tatoeba.org/exports/sentences.tar.bz2</a></p><p name="6e2e" id="6e2e" class="graf graf--p graf-after--p">I used a simple logic that combines a random number of sentences from the corpus and removes or changes the punctation and casing of the text to generate the text. The data generation scripts and pre-trained models will be available at <a href="https://github.com/bedapudi6788/Deep-Segmentation/" data-href="https://github.com/bedapudi6788/Deep-Segmentation/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/bedapudi6788/Deep-Segmentation/</a>. The data generated can be found at <a href="https://drive.google.com/open?id=1inDBFHZA8pKhVdFB-I4Vkk3tEuxzt6Dv" data-href="https://drive.google.com/open?id=1inDBFHZA8pKhVdFB-I4Vkk3tEuxzt6Dv" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://drive.google.com/open?id=1inDBFHZA8pKhVdFB-I4Vkk3tEuxzt6Dv</a></p><p name="0f03" id="0f03" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Deciding on the Model:</strong></p><p name="6bdf" id="6bdf" class="graf graf--p graf-after--p">After some thinking about the architecture(s) that would be best suited for this task, I decided that I will be able to use sequence to sequence models or sequence tagging models for the task at hand.</p><p name="e2c4" id="e2c4" class="graf graf--p graf-after--p">If I decide to go with sequence to sequence models, the data needs to be very huge, as in tens of millions of sentences, since the model needs to learn the context of each character from scratch and the training time would be very high. Leveraging pre-trained word embeddings such as Glove/ELMo won’t be possible or will be very tough to train. There is also another approach to this problem, where we restore punctuation through seq2seq models and use general segmentation techniques. This approach isn’t scalable, as training seq2seq would require lot of data and it will become exponentially tougher to deal with large texts. In fact, the correct way of doing punctuation restoration would be the exact reverse. i.e: Perform sentence segmentation on the unpunctuated text and use seq2seq for punctuation correction at a sentence level instead of on the whole text.</p><p name="bfea" id="bfea" class="graf graf--p graf-after--p">So, I decided to start with a BiLSTM+CRF sequence tagging model (<a href="https://github.com/bedapudi6788/seqtag" data-href="https://github.com/bedapudi6788/seqtag" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/bedapudi6788/seqtag</a>) along with pre-trained word embeddings and observe the results before planning on the next step. Although we can implement an arguably better performing model than this particular implementation (eg: Elmo + BiLSTM CRF or BERT + BiLSTM CRF), since the idea behind this is to get a baseline model as quickly as possible, I decided to start with the Glove + BiLSTM CRF implementation.</p><p name="60fd" id="60fd" class="graf graf--p graf-after--p">I started the training with 1 Million examples as the training data and 100,000 examples as validation data. I followed standard BIO format for labelling the data. The beginning of a sentence is labelled as “B-sent” and all other word are assigned the label “O”. So, for our example text “I am Batman I live in Gotham” the training data looks like</p><figure name="f4ab" id="f4ab" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 509px; max-height: 268px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 52.7%;"></div><img class="graf-image" data-image-id="1*G2SCxthPA6BwfFlpUc13Gw.png" data-width="509" data-height="268" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*G2SCxthPA6BwfFlpUc13Gw.png"></div></figure><p name="851f" id="851f" class="graf graf--p graf-after--figure">On the 1080Ti the training took ~24 minutes per epoch and reached convergence in 14 epochs at a validation F1 score of 0.9639.</p><p name="89f8" id="89f8" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Measuring the accuracy:</strong></p><p name="4b28" id="4b28" class="graf graf--p graf-after--p">Since the number 0.9721 F1 score doesn’t tell us much about the actual sentence segmentation accuracy in comparison to the existing algorithms, I devised the testing methodology as follows.</p><p name="1422" id="1422" class="graf graf--p graf-after--p">Test data:</p><ol class="postList"><li name="86f8" id="86f8" class="graf graf--li graf-after--p">1000 perfectly punctuated texts, each made up of 1–10 sentences with 0.5 probability of being lower cased (For comparison with spacy, nltk)</li><li name="b916" id="b916" class="graf graf--li graf-after--li">1000 texts with no punctuation, each made up of 1–10 sentences with 0.5 probability of being lower cased, (For measuring the accuracy of the model when dealing with text with no punctuation at all)</li><li name="cc7d" id="cc7d" class="graf graf--li graf-after--li">1000 texts with partially removed punctuation and with 0.5 probability of being lower cased (For measuring if the model is confused by bad punctuations)</li></ol><p name="a1b6" id="a1b6" class="graf graf--p graf-after--li">Note that these texts are created from random from 20000 sentences which were separated randomly.</p><p name="1d05" id="1d05" class="graf graf--p graf-after--p">The <strong class="markup--strong markup--p-strong">Absolute Accuracy</strong> in each case is scored as the <strong class="markup--strong markup--p-strong">total number of correctly segmented texts / total number of examples. </strong>i.e: A text is considered correctly segmented only if the module is able to split in to exactly the sentences into which it’s supposed to be split. The tester scripts are available in the git repo.</p><figure name="b0da" id="b0da" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 553px; max-height: 105px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 19%;"></div><img class="graf-image" data-image-id="1*jKsNbSK7_7xGWLVOyLjMew.png" data-width="553" data-height="105" src="https://cdn-images-1.medium.com/max/800/1*jKsNbSK7_7xGWLVOyLjMew.png"></div><figcaption class="imageCaption">Comparison of absolute accuracy</figcaption></figure><p name="d374" id="d374" class="graf graf--p graf-after--figure">DeepSegment achieved an average <strong class="markup--strong markup--p-strong">absolute accuracy of 73.35</strong> outperforming both Spacy and NLTK by a wide margin.</p><p name="841c" id="841c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Performance:</strong></p><p name="407a" id="407a" class="graf graf--p graf-after--p">DeepSegment took <strong class="markup--strong markup--p-strong">139.57 </strong>seconds to run on the entire dataset compared to NLTK’s <strong class="markup--strong markup--p-strong">0.53 </strong>seconds and Spacy’s <strong class="markup--strong markup--p-strong">54.63 </strong>seconds on a i5 dual core Macbook air. When ran on a modest 4 GB GTX 960M with batch inference (batch size 64), DeepSegment took <strong class="markup--strong markup--p-strong">2.6 seconds</strong> to run on the same test data. Note that Spacy and NLTK can’t benefit from the use of a GPU.</p><p name="bd95" id="bd95" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">UPDATE 1:</strong></p><p name="20da" id="20da" class="graf graf--p graf-after--p">Few people messaged me enquiring about the <strong class="markup--strong markup--p-strong">performance of DeepSegment for unpunctuated text</strong>. <strong class="markup--strong markup--p-strong">The reported 52.637% is the absolute accuracy score </strong>(see above for definition). I don’t like reporting precision/ recall wherever possible because, these scores make the accuracy look much higher than what it is. I honestly believe that, while building real world systems people should keep in mind that F1 score is always going to look much higher than what the actual accuracy is. (Not true for all cases, but in general).</p><p name="f4b9" id="f4b9" class="graf graf--p graf-after--p">For example take a look at the precision, recall and F1 scores of the above model.</p><figure name="e46f" id="e46f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 156px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 22.2%;"></div><img class="graf-image" data-image-id="1*cNEGY-DZPsTDASkhft8FBA.png" data-width="895" data-height="199" src="https://cdn-images-1.medium.com/max/800/1*cNEGY-DZPsTDASkhft8FBA.png"></div><figcaption class="imageCaption">Precision, Recall and F1-scores for labels B-sent and O</figcaption></figure><p name="4e57" id="4e57" class="graf graf--p graf-after--figure">For the completely unpunctuated test case, the <strong class="markup--strong markup--p-strong">absolute accuracy is 52.637</strong> (as reported) and the <strong class="markup--strong markup--p-strong">F1 score for the label B-sent is 91.33</strong> (precision: 93.242, recall: 89.506). Similarly for other test cases the F1 score is much higher than absolute accuracy (can be seen in the image above).</p><p name="92b9" id="92b9" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Next steps:</strong></p><ol class="postList"><li name="0dff" id="0dff" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">DeepSegment: </strong>Improve DeepSegment with more training data and test with ELMo, BERT and FastText vectors.</li><li name="7e04" id="7e04" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">DeepCorrect: </strong>Implement punctuation restoration and correction using sequence to sequence networks.</li></ol><p name="e2a5" id="e2a5" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">UPDATE 2:</strong></p><p name="ba07" id="ba07" class="graf graf--p graf-after--p graf--trailing">Check out <a href="https://medium.com/@praneethbedapudi/deepsegment-2-0-multilingual-text-segmentation-with-vector-alignment-fd76ce62194f" data-href="https://medium.com/@praneethbedapudi/deepsegment-2-0-multilingual-text-segmentation-with-vector-alignment-fd76ce62194f" class="markup--anchor markup--p-anchor" rel="nofollow" target="_blank">https://medium.com/@praneethbedapudi/deepsegment-2-0-multilingual-text-segmentation-with-vector-alignment-fd76ce62194f</a> for multi-lingual segmentation with single model.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@praneethbedapudi" class="p-author h-card">Praneeth Bedapudi</a> on <a href="https://medium.com/p/a1dbc0db4e98"><time class="dt-published" datetime="2018-11-17T19:40:03.725Z">November 17, 2018</time></a>.</p><p><a href="https://medium.com/@praneethbedapudi/deepcorrection-1-sentence-segmentation-of-unpunctuated-text-a1dbc0db4e98" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on February 21, 2020.</p></footer></article></body></html>