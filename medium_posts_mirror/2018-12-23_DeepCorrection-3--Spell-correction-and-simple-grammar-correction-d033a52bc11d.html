<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>DeepCorrection 3: Spell correction and simple grammar correction</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">DeepCorrection 3: Spell correction and simple grammar correction</h1>
</header>
<section data-field="subtitle" class="p-summary">
Spell correction using seq2seq models is nothing new. Tal Weiss tried to tackle spell correction by training a seq2seq model on data…
</section>
<section data-field="body" class="e-content">
<section name="9e3e" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="9c26" id="9c26" class="graf graf--h3 graf--leading graf--title">DeepCorrection 3: Spell correction and simple grammar correction</h3><p name="5488" id="5488" class="graf graf--p graf-after--h3">Spell correction using seq2seq models is nothing new. <a href="https://medium.com/u/39e124dd6c22" data-href="https://medium.com/u/39e124dd6c22" data-anchor-type="2" data-user-id="39e124dd6c22" data-action-value="39e124dd6c22" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Tal Weiss</a> tried to tackle spell correction by training a seq2seq model on data generated from Google news corpus. His <a href="https://github.com/MajorTal/DeepSpell" data-href="https://github.com/MajorTal/DeepSpell" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">original implementation</a> though it doesn’t work, inspired many to tackle spell correction with seq2seq. There were some unsuccessful attempts by others (<a href="https://medium.com/scribd-data-science-engineering/neural-spelling-corrections-and-the-importance-of-accuracy-977c0063d20f" data-href="https://medium.com/scribd-data-science-engineering/neural-spelling-corrections-and-the-importance-of-accuracy-977c0063d20f" class="markup--anchor markup--p-anchor" target="_blank">Matthew Relich</a>, <a href="https://medium.com/@surmenok/improving-deepspell-code-bdaab1c5fb7e" data-href="https://medium.com/@surmenok/improving-deepspell-code-bdaab1c5fb7e" class="markup--anchor markup--p-anchor" target="_blank">Pavel Surmenok</a>) at this.</p><p name="de9d" id="de9d" class="graf graf--p graf-after--p">Unfazed by this I decided to try and build a working spell corrector with deep learning. Initially I identified the major problems with the above implementations.</p><ol class="postList"><li name="0f8a" id="0f8a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Use of attention</strong>: Attention is very important for any sequence to sequence task. Without attention, the network needs to look at the entire input while making predictions. So, I built a seq2seq model with LSTM encoder, decoders and “Luong” attention. I used the same model for punctuation correction before and achieved excellent results in my <a href="https://medium.com/@praneethbedapudi/deepcorrection2-automatic-punctuation-restoration-ac4a837d92d9" data-href="https://medium.com/@praneethbedapudi/deepcorrection2-automatic-punctuation-restoration-ac4a837d92d9" class="markup--anchor markup--li-anchor" target="_blank">previous post</a>.</li><li name="e1a8" id="e1a8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Training Data:</strong> Instead of relying just on generation of data, I felt my goal would be better achieved by adding real world spelling mistakes too. So, I collected some data that reflects real world spelling mistakes and for generating training data I built a clean text corpus of roughly 80 million sentences.</li></ol><p name="7b9d" id="7b9d" class="graf graf--p graf-after--li">For the generation of data, I used a per word edit distance limit of 0.3 and also introduced real world spelling mistakes.</p><p name="bc85" id="bc85" class="graf graf--p graf-after--p">After 37 hours of training on a GTX 1080Ti the model achieved 0.99907 validation accuracy and the sample predictions looked excellent.</p><figure name="2555" id="2555" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 190px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 27.1%;"></div><img class="graf-image" data-image-id="1*Ae70TeBrnjOIfWB4LgJAIw.png" data-width="772" data-height="209" src="https://cdn-images-1.medium.com/max/800/1*Ae70TeBrnjOIfWB4LgJAIw.png"></div><figcaption class="imageCaption">Epoch 5 (each epoch on 80 million sentences)</figcaption></figure><p name="7df8" id="7df8" class="graf graf--p graf-after--figure">On the held out test data (30000 sentences), this model achieved an 0.9892 absolute accuracy, which is excellent. Just to real world test this mode, I hosted it on a test server and asked some of my friends to give it a try. I was pretty sure it was gonna work amazing.</p><p name="3218" id="3218" class="graf graf--p graf-after--p">To my dismay,<strong class="markup--strong markup--p-strong"> </strong>the response from the “testers” was pretty underwhelming. Most of them reported the same thing. The model was excellent at contextual spell correction but, it sucked at recognising names of people and some other trivial things.</p><p name="3319" id="3319" class="graf graf--p graf-after--p">Some examples:</p><ol class="postList"><li name="eaeb" id="eaeb" class="graf graf--li graf--startsWithDoubleQuote graf-after--p">“i wll b there for u” → “i will be there for you”</li><li name="4456" id="4456" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“these is not that gre at” → “this is not that great”</li><li name="5b38" id="5b38" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“i dotlike this” → “i dont like this”</li></ol><p name="472e" id="472e" class="graf graf--p graf-after--li">For instance, it changed</p><ol class="postList"><li name="8e5a" id="8e5a" class="graf graf--li graf--startsWithDoubleQuote graf-after--p">“i am jayadeep” →“i am a jay deep”</li><li name="a35a" id="a35a" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“i work at reckonsys” → “i work in reckoning”</li><li name="5097" id="5097" class="graf graf--li graf--startsWithDoubleQuote graf-after--li">“bedapudi works at abzooba he is a great person” → “beds works about he is a great person.”</li></ol><p name="6bda" id="6bda" class="graf graf--p graf-after--li">Even though the network’s precision and recall were excellent on the held-out data, on unseen data false positive prediction was higher than ideal. This indicated that for unseen words, the model’s predictions were bad. One of the reasons being, the model isn’t able to understand that proper nouns shouldn’t be spell corrected.</p><p name="4552" id="4552" class="graf graf--p graf-after--p">This is because, while generating the training data we introduced errors in proper nouns too, resulting in the model memorising the “corrected” proper nouns and trying to correct unseen words into some of the memorised words. This could most probably be corrected by either applying a max edit distance rule for each word while decoding or by not introducing errors for proper nouns while training. Using logic based decoding defeats the purpose of using DL and teaching the model not to correct proper nouns would result in the model being not able to correct simple mistakes like “jahn” → “john”.</p><p name="f6a4" id="f6a4" class="graf graf--p graf-after--p">This made me re-evaluate the approach of data generation for spelling correction and realise that for generic spelling correction, going with classical i.e: edit distance based models might be a better idea.</p><p name="101c" id="101c" class="graf graf--p graf-after--p">I evaluated <strong class="markup--strong markup--p-strong">SymSpell</strong> and <strong class="markup--strong markup--p-strong">JamSpell</strong> and the results were excellent. Though they cannot do (good) contextual correction, for simple spell correction these libraries were simply phenomenal. Especially, SymSpell by <a href="https://medium.com/u/66bab1f6efa2" data-href="https://medium.com/u/66bab1f6efa2" data-anchor-type="2" data-user-id="66bab1f6efa2" data-action-value="66bab1f6efa2" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Wolf Garbe</a> is blazing fast and can perform word segmentation too. But, these algorithms can’t effectively correct homonyms and other simple contextual errors (I ate a apple → I ate an apple).</p><p name="2623" id="2623" class="graf graf--p graf-after--p">Alex Paino’s<strong class="markup--strong markup--p-strong"> </strong><a href="https://github.com/atpaino/deep-text-corrector" data-href="https://github.com/atpaino/deep-text-corrector" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Deep Text Corrector</strong></a> serves as an <strong class="markup--strong markup--p-strong">excellent proof of concept</strong> for context based text correction. But, it’s functionality is very limited because</p><ol class="postList"><li name="5334" id="5334" class="graf graf--li graf-after--p">It is a word level model (Thus limiting it’s ability to expand to unseen words and making training on large datasets very very tough)</li><li name="a3c6" id="a3c6" class="graf graf--li graf-after--li">It uses a logic based decoding step where the word predicted is ignored if it’s not present in a set of pre-defined corrective tokens.</li><li name="c658" id="c658" class="graf graf--li graf-after--li">The data generation used was limited to article correction and very few homophone correction.</li></ol><p name="1c34" id="1c34" class="graf graf--p graf-after--li">Planning to expand on this, I created a correction dataset by introducing homonym, homophone and common grammatical errors. Since I already exhausted my resources for training the spell correction model, I was only able to train the model on data generated from 1.4 million sentences (Tatoeba + Cornell movie lines) for 5 hours.</p><figure name="bdc1" id="bdc1" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 689px; max-height: 451px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 65.5%;"></div><img class="graf-image" data-image-id="1*3TbI15eNSofflAu-Bq0cUQ.png" data-width="689" data-height="451" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*3TbI15eNSofflAu-Bq0cUQ.png"></div><figcaption class="imageCaption">Some example predictions</figcaption></figure><p name="e2e7" id="e2e7" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">The Testing:</strong></p><p name="ca7d" id="ca7d" class="graf graf--p graf-after--p">Since I trained the model on Tatoeba and Cornell, I tested the model with data generated from Google news 2013 corpus. This model achieved 0.8921 absolute accuracy (no of perfectly corrected lines/ total no of lines). Since the data generation technique is the same for training and testing data I followed the same method of testing I used for spell correction. Of the 134 sentences tried by real world users, the model was able to perfectly correct the grammar in 62 sentences and punctuation in 131 sentences. Of the 72 sentences where the model failed to correct grammar, 48 sentences were “out of scope”. i.e: They were verb form, plural singular errors.</p><p name="6f6d" id="6f6d" class="graf graf--p graf-after--p">I plan to train the same model on the 80 million sentence dataset that I curated along with further additions to the scope of correction including word form correction.</p><p name="a7b7" id="a7b7" class="graf graf--p graf-after--p graf--trailing">The code and pre-trained models are available at <a href="https://github.com/bedapudi6788/deepcorrect" data-href="https://github.com/bedapudi6788/deepcorrect" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://github.com/bedapudi6788/deepcorrect</a>. (The data and the pre-trained model will be made available by 25-Dec. I am currently trying to re-train the model with some more data). The demo will be available at <a href="http://bpraneeth.com/projects" data-href="http://bpraneeth.com/projects" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">http://bpraneeth.com/projects</a> after I train the model on 80 million sentences.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@praneethbedapudi" class="p-author h-card">Praneeth Bedapudi</a> on <a href="https://medium.com/p/d033a52bc11d"><time class="dt-published" datetime="2018-12-23T16:34:12.493Z">December 23, 2018</time></a>.</p><p><a href="https://medium.com/@praneethbedapudi/deepcorrection-3-spell-correction-and-simple-grammar-correction-d033a52bc11d" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on February 21, 2020.</p></footer></article></body></html>